{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Output Regression for Emotion Prediction\n",
        "Antony John Edathattil - aedatha@ncsu.edu\n",
        "\n",
        "Hritwik Shukla - hshukla@ncsu.edu\n",
        "\n",
        "Sahil Palarpwar - spalarp@ncsu.edu\n",
        "\n",
        "Link to Dataset: [https://github.com/antonyjohne/MultiOutputEmotionRegression](https://github.com/antonyjohne/MultiOutputEmotionRegression)\n"
      ],
      "metadata": {
        "id": "h8g7nLAvFNVl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "oOChzWDHavzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a71dc165-4ccf-491a-f0d3-333c0d4820e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tsfresh in /usr/local/lib/python3.9/dist-packages (0.20.0)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.9/dist-packages (from tsfresh) (0.13.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.9/dist-packages (from tsfresh) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.9/dist-packages (from tsfresh) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.9/dist-packages (from tsfresh) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.10.0 in /usr/local/lib/python3.9/dist-packages (from tsfresh) (4.65.0)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tsfresh) (0.5.3)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.9/dist-packages (from tsfresh) (2.27.1)\n",
            "Requirement already satisfied: stumpy>=1.7.2 in /usr/local/lib/python3.9/dist-packages (from tsfresh) (1.11.1)\n",
            "Requirement already satisfied: distributed>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tsfresh) (2022.12.1)\n",
            "Requirement already satisfied: dask[dataframe]>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tsfresh) (2022.12.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from tsfresh) (1.10.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.9/dist-packages (from tsfresh) (2.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from dask[dataframe]>=2.9.0->tsfresh) (23.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from dask[dataframe]>=2.9.0->tsfresh) (8.1.3)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from dask[dataframe]>=2.9.0->tsfresh) (6.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from dask[dataframe]>=2.9.0->tsfresh) (2023.4.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.9/dist-packages (from dask[dataframe]>=2.9.0->tsfresh) (0.12.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.9/dist-packages (from dask[dataframe]>=2.9.0->tsfresh) (1.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from distributed>=2.11.0->tsfresh) (1.7.0)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from distributed>=2.11.0->tsfresh) (1.0.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.9/dist-packages (from distributed>=2.11.0->tsfresh) (2.4.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from distributed>=2.11.0->tsfresh) (1.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from distributed>=2.11.0->tsfresh) (3.1.2)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.9/dist-packages (from distributed>=2.11.0->tsfresh) (2.2.0)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from distributed>=2.11.0->tsfresh) (6.2)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.9/dist-packages (from distributed>=2.11.0->tsfresh) (5.9.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from distributed>=2.11.0->tsfresh) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.25.0->tsfresh) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.25.0->tsfresh) (2022.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy>=0.4.1->tsfresh) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.1->tsfresh) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.1->tsfresh) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.1->tsfresh) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.0->tsfresh) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.0->tsfresh) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.9/dist-packages (from stumpy>=1.7.2->tsfresh) (0.56.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.54->stumpy>=1.7.2->tsfresh) (67.7.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.54->stumpy>=1.7.2->tsfresh) (0.39.1)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.9/dist-packages (from zict>=0.1.3->distributed>=2.11.0->tsfresh) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->distributed>=2.11.0->tsfresh) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tsfresh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Connect to a TPU runtime in Google Colab to train the LSTM model\n",
        "Runtime --> Change Runtime Type --> Select TPU\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "PW7RB1FDrPvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020e2c0b-a15e-4658-bb05-7fb70071811b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.65.5.194:8470']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.65.5.194:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DsUiCsX_kBy",
        "outputId": "9de0690f-8be3-45b4-d370-dc508d4e3d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/Alda/Dataset\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Mount your Drive and initialize the data paths\n",
        "Ensure the dataset in within the folder named \"Dataset\" and the program is within\n",
        "the home directory outside the Dataset folder\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/Alda/Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "KwO9tTC-AVBX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import tsfresh as ts\n",
        "import matplotlib.pyplot as plt\n",
        "from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters\n",
        "\n",
        "import lightgbm as lgb\n",
        "from keras import optimizers\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.models import Model, Sequential\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, LSTM, Bidirectional\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "\"\"\"\n",
        "Data Format:\n",
        "â€¢\tData = [13, 2, ?] \n",
        "  - First dimension (12) = Number of videos\n",
        "  - Second dimension (2) = Number of Channels -\tChannels = [GSR, ECG]\n",
        "  - Third dimension (?) = Length of Data \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "signal_path = os.getcwd()+\"/05 ECG-GSR Data/01 ECG-GSR Data (Pre-Processed)\"\n",
        "scores_path = os.getcwd()+\"/03 Self-Reported Questionnaires\"\n",
        "os.makedirs(os.getcwd()+f\"/ProcessedData\", exist_ok=True)\n",
        "os.makedirs(os.getcwd()+f\"/FeatureData\", exist_ok=True)\n",
        "\n",
        "processed_data_path = os.getcwd()+'/ProcessedData'\n",
        "sampled_data_dir = os.getcwd()+'/FeatureData'\n",
        "\n",
        "score_df = pd.read_excel(scores_path + \"/02 Post Exposure Ratings.xlsx\")\n",
        "score_labels = score_df.columns[9:14]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "Inc8vmgsCkxh"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Read each subject --> Check for bad data -->  Extract 120 sec of signal data\n",
        "Convert Raw Data to DataFrame and export as file with name \"SubID_VideoID.csv\"\n",
        "\"\"\"\n",
        "\n",
        "def x_dataloader(num_subjects):\n",
        "\n",
        "  os.makedirs(os.getcwd()+f\"/ProcessedData\", exist_ok=True)\n",
        "  signal_file_list = os.listdir(signal_path)\n",
        "  print(\"Data Preprocessing has begun. This might take a while...!\\n\")\n",
        "\n",
        "  for sub in range(num_subjects):\n",
        "    if sub == 6 or sub == 26: #Bad Subject Data\n",
        "      continue\n",
        "\n",
        "    filename = signal_path + '/' + signal_file_list[sub]\n",
        "\n",
        "    with open(filename,  'rb')  as  subject_file:\n",
        "      data = pickle.Unpickler(subject_file).load()\n",
        "\n",
        "      for vid_num in range(12):\n",
        "        try:\n",
        "          signal_df = pd.DataFrame(np.array(data[\"Data\"][vid_num][:120000]), columns=['GSR', 'ECG'])\n",
        "          signal_df.insert(0, 'time', np.arange(0.001, 120.0001, 0.001), allow_duplicates=True)\n",
        "          signal_df.insert(0, 'id', sub+101, allow_duplicates=True)\n",
        "          signal_df.insert(1, 'vid_num', vid_num+1, allow_duplicates=True)\n",
        "          signal_df.fillna(0.0, inplace=True)\n",
        "          signal_df.to_csv(f\"{os.getcwd()}/ProcessedData/{sub+101}_V{vid_num+1}.csv\", header=True, index=False)\n",
        "        \n",
        "        except IndexError as e:\n",
        "          print(f\"\\nError while Processing Subject {sub}, Video {vid_num}\")\n",
        "    print(f\"Subject {sub} Data Preprocessed Successfully!\")\n",
        "  print(\"\\nData Preprocessed Successfully!\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "oU_eKDgFd8ll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee3e757-a696-44b8-9d5d-534d280d721a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tsfresh.feature_extraction.settings:Dependency not available for matrix_profile, this feature will be disabled!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Create feature windows for Time series data every 4 sec --> Extract Statistical Features of window\n",
        "Export features per subject per vido into folder \"FeatureData\"\n",
        "\"\"\"\n",
        "\n",
        "def feature_engineering(window_size=4000, step_size=4000, parameter_set=MinimalFCParameters(), param_set_name='Minimal'):\n",
        "\n",
        "  # Define the path to the directory where the CSV files are stored\n",
        "  storage_path = sampled_data_dir+f\"/{param_set_name}\"\n",
        "  os.makedirs(storage_path, exist_ok=True)\n",
        "\n",
        "  # Loop through all the CSV files in the directory\n",
        "  for filename in os.listdir(processed_data_path):\n",
        "      if filename.endswith(\".csv\"):\n",
        "          # Read the CSV file using pandas\n",
        "          df = pd.read_csv(os.path.join(processed_data_path, filename))\n",
        "\n",
        "          for j in range(0, df.shape[0] - window_size, step_size):\n",
        "              temp_list = df.iloc[j: j + window_size, :]\n",
        "\n",
        "              window_filename = f\"{filename[:-4]}_sampled.csv\"\n",
        "\n",
        "              extracted_features_df = ts.extract_features(temp_list, column_id='id', column_sort='time', default_fc_parameters = parameter_set)\n",
        "              extracted_features_df.to_csv(storage_path+'/'+window_filename, index=False, mode='a', header=not os.path.exists(storage_path+'/'+window_filename))\n",
        "\n",
        "  print(\"Features have been Engineered into folder 'Feature Data'\")\n",
        "  return os.listdir(sampled_data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "73JumD9vIa8M"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Read 5 emotion score labels for each SubID_VidID and match to Feature Windows\n",
        "Create a complete dataset with Features & Labels and export\n",
        "\"\"\"\n",
        "def final_dataset_compiler(param_set_name):\n",
        "    temp_feature_list = []\n",
        "    storage_path = sampled_data_dir+f\"/{param_set_name}\"\n",
        "\n",
        "    feature_file_list = os.listdir(storage_path)\n",
        "\n",
        "    for i, feature_file in enumerate(feature_file_list):\n",
        "      filename_split = feature_file.split(\"_\")\n",
        "      subject_id, vid_num = int(filename_split[0]), int(filename_split[1][1:])\n",
        "\n",
        "      filename = os.path.join(storage_path, feature_file)\n",
        "      temp_df = pd.read_csv(filename)\n",
        "      emotion_scores = score_df[(score_df[\"ID\"]==subject_id) & (score_df[\"Num_Code\"]==vid_num)][score_labels]\n",
        "\n",
        "      for label in emotion_scores.columns:\n",
        "        temp_df.insert(temp_df.shape[1], str(label), float(emotion_scores[label]), allow_duplicates=True)\n",
        "\n",
        "      temp_feature_list.append(temp_df)\n",
        "\n",
        "    df = pd.concat(temp_feature_list)\n",
        "    df = df.dropna()\n",
        "    df.to_csv(f\"FinalDataset_{param_set_name}.csv\", header=True)\n",
        "    print(f\"{param_set_name} Dataset Shape: {df.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "fHwkLgTDNiW5"
      },
      "outputs": [],
      "source": [
        "def metric_calc(y_pred, y_true):\n",
        "    mae =  mean_absolute_error(y_true, y_pred)\n",
        "    rmse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"Metrics:\\n   MAE: {mae}   |   MSE: {rmse}   |   R2:{r2}\\n\")\n",
        "    return mae, rmse, r2\n",
        "\n",
        "def feature_selection(x, plots=True, num_feat=2):\n",
        "    pca = PCA()\n",
        "    x_pca = pca.fit_transform(x)\n",
        "    features = range(pca.n_components_)\n",
        "    if plots:\n",
        "        plt.figure(figsize=(6, 3))\n",
        "        plt.bar(features, pca.explained_variance_)\n",
        "        plt.xlabel('PCA feature')\n",
        "        plt.ylabel('Variance')\n",
        "        plt.xticks(features)\n",
        "        plt.show()\n",
        "    pca = PCA(n_components=num_feat)\n",
        "    x_pruned = pca.fit_transform(x)\n",
        "    return x_pruned\n",
        "\n",
        "def LSTM_feature_engineering(window_size=4000, step_size=4000):\n",
        "    \"\"\"\n",
        "    Create 4 sec signal windows of features & classes using processed dataset\n",
        "    Transform into LSTM input shape: (samples, timesteps, features)\n",
        "    \"\"\"\n",
        "    x_data, y_data = [], []\n",
        "\n",
        "    for filename in os.listdir(processed_data_path):\n",
        "        if filename.endswith(\".csv\"):\n",
        "            df = pd.read_csv(os.path.join(processed_data_path, filename))\n",
        "            standard_df = StandardScaler().fit_transform(df.loc[:,['GSR','ECG']])\n",
        "            for j in range(0, df.shape[0] - window_size, step_size):\n",
        "                temp_list = standard_df[j: j + window_size]\n",
        "                temp_labels = score_df[(score_df[\"ID\"]==df['id'][0]) & (score_df[\"Num_Code\"]==df['vid_num'][0])][score_labels]\n",
        "                x_data.append(temp_list)\n",
        "                y_data.append(temp_labels)\n",
        "\n",
        "    print(\"\\nFeatures have been windowed into the correct shape for LSTM input.\\n\")\n",
        "    return np.array(x_data), np.array(y_data)\n",
        "\n",
        "\n",
        "def LRegression(x_train, y_train):\n",
        "    print(\"\\nLasso Regressor...\\nFitting Model. Please Wait...\")\n",
        "    lmodel = Lasso(random_state=1306)\n",
        "    lmodel.fit(x_train, y_train)\n",
        "    print(\"STATUS - Model Fit Complete\")\n",
        "    return lmodel\n",
        "\n",
        "\n",
        "def RFRegression(x_train, y_train):\n",
        "    print(\"\\nRandom Forest Regressor...\\nFitting Model. Please Wait...\")\n",
        "    rfmodel = RandomForestRegressor(max_depth=10, random_state=1306)\n",
        "    rfmodel.fit(x_train, y_train)\n",
        "    print(\"STATUS - Model Fit Complete\")\n",
        "    return rfmodel\n",
        "\n",
        "def MLPRegression(x_train, y_train):\n",
        "    print(\"\\nMLP Regressor...\\nFitting Model. Please Wait...\")\n",
        "    mlpmodel = MLPRegressor(hidden_layer_sizes=(30,30,10), activation='relu', solver='adam', alpha=0.1, batch_size='auto', max_iter=1000, random_state=13)\n",
        "    mlpmodel.fit(x_train, y_train)\n",
        "    print(\"STATUS - Model Fit Complete\")\n",
        "    return mlpmodel\n",
        "\n",
        "def LGBMRegression1(x_train, y_train):\n",
        "    print(\"\\nLightGBM Regressor 1...\\nFitting Model. Please Wait...\")\n",
        "    lgbmmodel = MultiOutputRegressor(lgb.LGBMRegressor(num_leaves=100, max_depth=-1,n_estimators=200,learning_rate=0.01))\n",
        "    lgbmmodel.fit(x_train, y_train)\n",
        "    print(\"STATUS - Model Fit Complete\")\n",
        "    return lgbmmodel  \n",
        "    \n",
        "def LGBMRegression2(x_train, y_train):\n",
        "    print(\"\\nLightGBM Regressor 2...\\nFitting Model. Please Wait...\")\n",
        "    lgbmmodel = MultiOutputRegressor(lgb.LGBMRegressor(num_leaves=200, max_depth=-1,n_estimators=100,learning_rate=0.05))\n",
        "    lgbmmodel.fit(x_train, y_train)\n",
        "    print(\"STATUS - Model Fit Complete\")\n",
        "    return lgbmmodel\n",
        "\n",
        "def LGBMRegression3(x_train, y_train):\n",
        "    print(\"\\nLightGBM Regressor 3...\\nFitting Model. Please Wait...\")\n",
        "    lgbmmodel = MultiOutputRegressor(lgb.LGBMRegressor(num_leaves=300, max_depth=-1,n_estimators=200,learning_rate=0.05))\n",
        "    lgbmmodel.fit(x_train, y_train)\n",
        "    print(\"STATUS - Model Fit Complete\")\n",
        "    return lgbmmodel  \n",
        "\n",
        "def XGBRegression(x_train, y_train):\n",
        "    print(\"\\nXGBoost Regressor...\\nFitting Model. Please Wait...\")\n",
        "    xgbmodel = XGBRegressor(booster='gbtree')\n",
        "    xgbmodel.fit(x_train, y_train)\n",
        "    print(\"STATUS - Model Fit Complete\")\n",
        "    return xgbmodel \n",
        "\n",
        "def LSTMRegression(x_train, y_train):\n",
        "    print(\"\\nLSTM Regressor...\\nFitting Model. Please Wait...\")\n",
        "    with tpu_strategy.scope():\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(8, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
        "        model.add(Dense(5))\n",
        "        optimizer = optimizers.Adam(learning_rate=0.001)\n",
        "        model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy']) \n",
        "    model.fit(x_train, y_train, batch_size=16, epochs=10, verbose=1)\n",
        "    print(\"STATUS - Model Fit Complete\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojPRXLNBxqL0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Generate the final dataset to run models on if dataset doesn't exist\n",
        "\"\"\"\n",
        "if 'FinalDataset_Minimal.csv' not in os.list_dir(os.getcwd()):\n",
        "    x_dataloader(34)\n",
        "    feature_engineering(window_size=4000, step_size=4000, parameter_set=MinimalFCParameters(), param_set_name='Minimal')\n",
        "    final_dataset_compiler(param_set_name='Minimal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oPRUkICLtO8"
      },
      "outputs": [],
      "source": [
        "for i in range(2):\n",
        "\n",
        "    if i==0:\n",
        "        print(\"---------- MODEL STUDY USING MINIMAL DATASET ----------\")\n",
        "        df = pd.read_csv(\"FinalDataset_Minimal.csv\")\n",
        "        mask = df.applymap(lambda x: pd.to_numeric(x, errors='coerce')).isnull().any(axis=1)\n",
        "        df = df[~mask]\n",
        "        X, Y = df.iloc[:,:-5], df.iloc[:,-5:]\n",
        "\n",
        "    elif i==1:\n",
        "        print(\"\\n\\n---------- MODEL STUDY USING MINIMAL DATASET WITH PCA ----------\")\n",
        "        df = pd.read_csv(\"FinalDataset_Minimal.csv\")\n",
        "        mask = df.applymap(lambda x: pd.to_numeric(x, errors='coerce')).isnull().any(axis=1)\n",
        "        df = df[~mask]\n",
        "        X, Y = df.iloc[:,:-5], df.iloc[:,-5:]\n",
        "        X = feature_selection(X, plots=True, num_feat=5)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=13)\n",
        "    print(f\"X_Train Shape: {x_train.shape}, Y_Train Shape: {y_train.shape}, X_Test Shape: {x_test.shape}, Y_Test Shape:{y_test.shape}\")\n",
        "    scaler = StandardScaler().fit(x_train)\n",
        "    x_train, x_test = scaler.transform(x_train), scaler.transform(x_test)\n",
        "\n",
        "    models_dict= {'RF':RFRegression, 'MLP':MLPRegression, 'L':LRegression, 'LGBM1':LGBMRegression1, \n",
        "                  'LGBM2':LGBMRegression2, 'LGBM3':LGBMRegression3, 'XGB': XGBRegression, 'LSTM': LSTMRegression}\n",
        "\n",
        "    for model_name in models_dict.keys():\n",
        "        if model_name == 'LSTM':\n",
        "            X , Y =  LSTM_feature_engineering(window_size=1000, step_size=1000)\n",
        "            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=13)\n",
        "\n",
        "            x_train_og_shape, x_test_og_shape = x_train.shape, x_test.shape\n",
        "            x_train, x_test = np.reshape(x_train, (x_train.shape[0], -1)), np.reshape(x_test, (x_test.shape[0], -1))\n",
        "\n",
        "            scaler = StandardScaler().fit(x_train)\n",
        "            x_train, x_test = scaler.transform(x_train), scaler.transform(x_test)\n",
        "\n",
        "            x_train, x_test = np.reshape(x_train, (-1,x_train_og_shape[1],x_train_og_shape[2])),  np.reshape(x_test, (-1,x_test_og_shape[1],x_test_og_shape[2]))\n",
        "\n",
        "        model = models_dict[model_name](x_train, y_train)\n",
        "        y_pred = np.ceil(model.predict(x_test)).clip(min=0)\n",
        "        mae, rmse, r2 = metric_calc(y_pred, y_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}